{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PR-GfAB08C8Q",
        "outputId": "19d4b6b0-9977-4f38-d794-fa3df2241b6d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=7d4a284c4bbc69705f9c23406badfefea82f2f7f7dec10085c058e121df87928\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip show pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRCzcK_o74A_",
        "outputId": "0aa2f317-c028-4aad-a741-5d566cafcafc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: pyspark\n",
            "Version: 3.5.0\n",
            "Summary: Apache Spark Python API\n",
            "Home-page: https://github.com/apache/spark/tree/master/python\n",
            "Author: Spark Developers\n",
            "Author-email: dev@spark.apache.org\n",
            "License: http://www.apache.org/licenses/LICENSE-2.0\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: py4j\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PySparkを起動\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('myApp').getOrCreate()\n",
        "\n",
        "# DataFrameを作成\n",
        "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
        "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
        "\n",
        "# DataFrameを表示\n",
        "df.show()"
      ],
      "metadata": {
        "id": "SSEVr_dqr0ck",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ca20c63-0595-46bb-d827-c81842390538"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   Name|Age|\n",
            "+-------+---+\n",
            "|  Alice|  1|\n",
            "|    Bob|  2|\n",
            "|Charlie|  3|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import FloatType, TimestampType, StringType\n",
        "\n",
        "\n",
        "df = spark.createDataFrame([[1, 2, 3], [2, 3, 4], [4, 5, 6]],\n",
        "                           ['id', 'a', 'b'])\n",
        "df2 = spark.createDataFrame([[1], [2]], ['id'])\n",
        "\n",
        "df.show()\n",
        "\n",
        "df2.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Trcrexw18e0b",
        "outputId": "96e4541f-b490-4b8d-ff44-28e4418387bd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+\n",
            "| id|  a|  b|\n",
            "+---+---+---+\n",
            "|  1|  2|  3|\n",
            "|  2|  3|  4|\n",
            "|  4|  5|  6|\n",
            "+---+---+---+\n",
            "\n",
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  1|\n",
            "|  2|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sdf = spark.createDataFrame([[1, 'abc', 123], [2, 'aaa', 456], [4, 'bbb', 678]],\n",
        "                           ['id', 'a', 'b'])"
      ],
      "metadata": {
        "id": "lc8A21uai-Zf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data2 = [(3, 'red', 234),\n",
        "         (5, 'yellow', 234),\n",
        "         (6, 'ccc', 123),\n",
        "         (7, 'a', 123),\n",
        "         (8, 'b', 123),\n",
        "         (9, 'c', 123),\n",
        "         (10, 'd', 123),\n",
        "         (11, 'e', 234),\n",
        "         (12, 'f', 234),\n",
        "         (13, 'g', 123),\n",
        "         (14, 'h', 123),\n",
        "         (15, 'i', 123),\n",
        "         (16, 'j', 123),\n",
        "         (17, 'k', 123),\n",
        "         (18, 'l', 123),\n",
        "         (19, 'm', 123),\n",
        "         (20, 'n', 123),\n",
        "         (21, 'o', 234),\n",
        "         (22, 'p', 234),\n",
        "         (23, 'q', 123),\n",
        "         (24, 'r', 123),\n",
        "         (25, 's', 123),\n",
        "         (26, 't', 123),\n",
        "         (27, 'u', 123),\n",
        "         (28, 'v', 123),\n",
        "         (29, 'w', 123),\n",
        "         (30, 'x', 123),\n",
        "         (31, 'y', 123),\n",
        "         ]\n",
        "\n",
        "sdf2 = spark.createDataFrame(data2, ['id', 'a', 'b'])\n",
        "\n",
        "sdf = sdf.union(sdf2)\n",
        "\n",
        "display(sdf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BfuVwunviNAN",
        "outputId": "e7092998-3612-4af8-8421-eed30f2afa45"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, a: string, b: bigint]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sdf.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrIF9mNYlCux",
        "outputId": "3b298cb9-b9b4-4c96-edb6-1cd77b4d6376"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---+\n",
            "| id|     a|  b|\n",
            "+---+------+---+\n",
            "|  1|   abc|123|\n",
            "|  2|   aaa|456|\n",
            "|  4|   bbb|678|\n",
            "|  3|   red|234|\n",
            "|  5|yellow|234|\n",
            "|  6|   ccc|123|\n",
            "|  7|     a|123|\n",
            "|  8|     b|123|\n",
            "|  9|     c|123|\n",
            "| 10|     d|123|\n",
            "| 11|     e|234|\n",
            "| 12|     f|234|\n",
            "| 13|     g|123|\n",
            "| 14|     h|123|\n",
            "| 15|     i|123|\n",
            "| 16|     j|123|\n",
            "| 17|     k|123|\n",
            "| 18|     l|123|\n",
            "| 19|     m|123|\n",
            "| 20|     n|123|\n",
            "+---+------+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sdf.filter(\n",
        "F.col(\"id\").isin(range(20, 30))\n",
        ")"
      ],
      "metadata": {
        "id": "9VErwOGG85j2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "ccf2dacb-ed38-42ab-d0e3-02523f9f05f8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SparkRuntimeException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSparkRuntimeException\u001b[0m                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-3db942e00f8f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m sdf.filter(\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36misin\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    970\u001b[0m         cols = cast(\n\u001b[1;32m    971\u001b[0m             \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_create_column_from_literal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m         )\n\u001b[1;32m    974\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_active_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    970\u001b[0m         cols = cast(\n\u001b[1;32m    971\u001b[0m             \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_create_column_from_literal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m         )\n\u001b[1;32m    974\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_active_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_create_column_from_literal\u001b[0;34m(literal)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_create_column_from_literal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliteral\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LiteralType\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DecimalLiteral\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Column\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_active_spark_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJVMView\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mliteral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSparkRuntimeException\u001b[0m: [UNSUPPORTED_FEATURE.LITERAL_TYPE] The feature is not supported: Literal for '[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]' of class java.util.ArrayList."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sdf.filter(\n",
        "F.col(\"id\").isin(list(range(20, 30)))\n",
        ").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Er3E_th4lGeL",
        "outputId": "9eb46b20-86c0-4077-c4cc-db242c18f73f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---+---+\n",
            "| id|  a|  b|\n",
            "+---+---+---+\n",
            "| 20|  n|123|\n",
            "| 21|  o|234|\n",
            "| 22|  p|234|\n",
            "| 23|  q|123|\n",
            "| 24|  r|123|\n",
            "| 25|  s|123|\n",
            "| 26|  t|123|\n",
            "| 27|  u|123|\n",
            "| 28|  v|123|\n",
            "| 29|  w|123|\n",
            "+---+---+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2J4J9RT-llC0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}